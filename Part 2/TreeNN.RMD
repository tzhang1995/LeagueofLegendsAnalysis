---
title: "Trees and Support Vector Machines"
output: html_notebook
---

This notebook outlines my process of tree based and Neural Network models. This notebook is dependent on the data table gameInfo generated from DataExtraction.RMD.

# Loading Data from other part
```{r}
load("../data/league.RDATA")
```


# Packages
```{r}
library(tidyverse)
library(data.table)
library(randomForest)
library(rpart.plot)
library(word2vec)
library(Rtsne)
library(plotly)
library(keras)
library(tfruns)
library(rsample)
```

# List to Store Results
```{r}
data.tree <- list(
  models = list(),
  plots = list(),
  temp.data = list()
)
championCluster <- list(
  models = list(),
  plots = list(),
  temp.data = list()
)
```


# Wrangling Data
So I want to make a basic tree classifier of projected winning team comps. For now, a basic model of simple champion tags will be used.
```{r}
data.tree$temp.data$gameInfo.temp <- gameInfo %>% 
  left_join(
    champions.scraped,
    by = c("championName" = "name")
  ) %>% 
  group_by(match) %>% 
  mutate(
    team = rleid(win)
  ) %>% 
  ungroup()

data.tree$temp.data$gameInfo.tags <- data.tree$temp.data$gameInfo.temp %>% 
  group_by(match, team) %>% 
  count(tag) %>% 
  ungroup() %>% 
  pivot_wider(
    names_from = tag,
    values_from = n
  ) %>% 
  pivot_wider() %>% 
  replace(is.na(.), 0) 


data.tree$temp.data$gameInfo.tree <- data.tree$temp.data$gameInfo.temp %>% 
  filter(win == TRUE) %>% 
  select(match, team_win = team) %>% 
  distinct(match, .keep_all = T) %>% 
  mutate(
    team_win = factor(team_win, levels = c(1, 2))
  ) %>% 
  left_join(
    data.tree$temp.data$gameInfo.tags %>% 
      filter(team == 1) %>% 
      rename_with(
        .fn = function(x){
          
          paste0(x, "_1") %>% 
            return()
          
        },
        .cols = 3:8
      ) %>% 
      select(!team),
    by = "match"
  ) %>% 
  left_join(
    data.tree$temp.data$gameInfo.tags %>% 
      filter(team == 2) %>% 
      rename_with(
        .fn = function(x){
          
          paste0(x, "_2") %>% 
            return()
          
        },
        .cols = 3:8
      ) %>% 
      select(!team),
    by = "match"
  ) %>% 
  mutate_if(is.integer, as.factor)

data.tree$temp.data$gameInfo.tree
```

# Setting up Training / Test Data
```{r}
# Setting Seed for Reproducibility
set.seed(3)
# Next time use rsample 
data.tree$temp.data$sample <- sample(data.tree$temp.data$gameInfo.tree$match, nrow(data.tree$temp.data$gameInfo.tree)*.7)
data.tree$temp.data$train <- data.tree$temp.data$gameInfo.tree %>% 
  filter(match %in% data.tree$temp.data$sample)
data.tree$temp.data$test <- data.tree$temp.data$gameInfo.tree %>% 
  filter(!match %in% data.tree$temp.data$sample)
```

# Generating Random Forest
```{r}
set.seed(3)
data.tree$models$teamComp_forest <- randomForest(
  team_win ~ . - match,
  data = data.tree$temp.data$train,
  ntree = 500,
  importance = TRUE,
  na.action = na.omit
)

data.tree$models$teamComp_forest
```
```{r}
importance(data.tree$models$teamComp_forest)
varImpPlot(data.tree$models$teamComp_forest)
```
Let's compare to a simple blue side always wins classifier:
```{r}
data.tree$temp.data$gameInfo.tree %>% 
  count(team_win) %>% 
  mutate(n = n/sum(n))
```
Well, it's slightly better than the naive blue side win classifier but clearly the number of champions with tags isn't a very strong predictor of team success. With the current coding, I'm fairly certain that there won't really be a robust classifier.

Let's try to identify clusters of champion types.
# Generating Input Team Sentences 
```{r}
championCluster$temp.data$teams <- gameInfo %>% 
  select(match, win, championName) %>% 
  group_by(match, win) %>% 
  mutate(championNumber = row_number()) %>% 
  pivot_wider(
    names_from = championNumber,
    values_from = championName
  ) %>% 
  transmute(match = match, win = win, team = str_c(`1`,`2`,`3`,`4`,`5`, sep = " ")) %>% 
  ungroup() 

championCluster$temp.data$teams
```
# Generating Model
```{r}
set.seed(3)
championCluster$models$nlpModel <- word2vec(
  x = championCluster$temp.data$teams$team, 
  type = "skip-gram", 
  dim = 20, 
  iter = 15
)

# Embedding Matrix
championCluster$models$embeddingMatrix <- as.matrix(championCluster$models$nlpModel)

# Applying TSne 
championCluster$models$Tsne <- Rtsne(championCluster$models$embeddingMatrix, pca = FALSE)

championCluster$plots$map <- championCluster$models$Tsne$Y %>% 
  as.data.frame() %>%
  mutate(champion = row.names(championCluster$models$embeddingMatrix)) %>%
  ggplot(aes(x = V1, y = V2, label = champion)) + 
  geom_point() 

championCluster$plots$map <- championCluster$plots$map %>% 
  ggplotly()

championCluster$plots$map 
```
Pretty clearly 5 main clusters of champions each corresponding to a role. Doesn't really help too much in determining team compositions. I could set up a KNN to verify this but it seems pretty clear cut to me.

# Neural Network
## Wrangle Data
```{r}
data.NN <- list()
data.NN$data.temp <- championCluster$temp.data$teams %>% 
  select(!match)
  
data.NN$data.temp
```

# Running Model - See TeamCompNN.R
## Hyperparameter Tuning
```{r}
runs <- tuning_run(
  "TeamCompNN.R",
  flags = list(
    dropout = c(0.2, 0.3, 0.4, 0.5),
    unit = c(8, 16, 64)
  )
)

runs %>% 
  arrange(desc(metric_val_accuracy))
# So a dropout of .3 and 8 unit dense network seems to produce the best validation error
```
```{r}
results
```
52.14% accuracy, not the best, but not bad considering the variance of league of legends.

```{r}
model %>% predict("Camille Talon Veigar Jihn Lux")
```
A very weird way to code a team comp predictor - I'll try a different method in Part 3.
