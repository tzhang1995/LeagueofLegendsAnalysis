---
title: "Basic Statistical Models"
output: html_notebook
---

This notebook outlines my process of generating basic statistical models and exploratory data analysis. This notebook is dependent on the data table gameInfo generated from DataExtraction.RMD.

# Packages
```{r}
library(tidyverse)
library(sjPlot)
library(plotly)
```

# Empty Lists to Store Results
```{r}
# Don't run this again or else you'll clear the data, commented out to avoid this problem :<
# results <- list(
#   models = list(),
#   plots = list()
# )
```

# Vision Score
Let's examine the effect of vision score on predicted probability of winning. If the theory is true that controlling vision is integral for winning (which by all intuition it should be) then higher vision score should increase the log odds of victory.
## Just Vision Score
```{r}
results$models$visionScore <- glm(win ~ visionScore, family = "binomial", data = gameInfo)

summary(results$models$visionScore)
```
```{r}
results$plots$visionScore <- plot_model(results$models$visionScore, type = "pred", grid = F)

results$plots$visionScore
```
So as expected, increasing vision score *does* appear to increase your chances of victor. So for every point of vision score, the log odds of winning increases by 0.0096 and at a vision score of ~23.4, your win chance is 50%. But what if we wanted to include role (adc, support, etc.)? Is it more important for a particular role to have a high vision score? It should be noted that the average and standard deviation of vision score is 23.49 and 19.96 respectively, so the large vision scores above 100 are very rare and the win rate should not be taken as accurate.

## Include Role
```{r}
results$models$visionScore.role <- glm(win ~ visionScore*individualPosition, family = "binomial", data = gameInfo.noInvalid)

summary(results$models$visionScore.role)
```
```{r}
results$plots$visionScore.role <- plot_model(results$models$visionScore.role, type = "int")

results$plots$visionScore.role
```
As it turns out, having laners / junglers that ward is a very strong indicator of success. If your laners / jungler have a vision score of less than 25, they are *actively* trolling. Adding time is too big to handle on my local machine - instead I will divide the data into segments and run the individual logistic regressions. I assume since the effect of vision score is near uniform across the non support roles that a model of just the support role and just one other role should be sufficient.
## Include Game Time
```{r}
results$models$visionScore.time.support <- glm(win ~ visionScore*timePlayed, data = gameInfo %>% filter(individualPosition == "UTILITY"), family = "binomial")

results$models$visionScore.time.nonSupport <- glm(win ~ visionScore*timePlayed, data = gameInfo %>% filter(individualPosition == "BOTTOM"), family = "binomial")

summary(results$models$visionScore.time.support)
summary(results$models$visionScore.time.nonSupport)
```
## Constructing a Table to Plot Logistic Regression Surface
### Function to Generate Data in the Format Required by Plotly
```{r}
data.temp$visionScore <- expand_grid(
    visionScore = seq(min(gameInfo$visionScore), mean(gameInfo$visionScore) + 4*sd(gameInfo$visionScore), length.out = 50),
    timePlayed = seq(5, 60, length.out = 50)
  ) 

# NEXT TIME USE THE OUTER FUNCTION BUT OK
GET_SURFACE <- function(MODEL, TABLE = data.temp$visionScore){
  
  table.temp <- TABLE %>% 
    mutate(
      p_win = 1-exp(MODEL$coefficients[[1]]  + MODEL$coefficients[[2]]*visionScore + MODEL$coefficients[[3]]*timePlayed + MODEL$coefficients[[4]]*visionScore*timePlayed)/(1 + exp(MODEL$coefficients[[1]]  + MODEL$coefficients[[2]]*visionScore + MODEL$coefficients[[3]]*timePlayed + MODEL$coefficients[[4]]*visionScore*timePlayed)
      )
    )
  
  output.raw <- list()
  
  for(i in 0:49){
    
    output.raw[[i+1]] <- table.temp$p_win[(50*i+1):((i+1)*50)]
    
  }
  
  output <- rbind(output.raw[1:50]) %>% 
    return()
  
}

data.temp$visionScore.support <- GET_SURFACE(results$models$visionScore.time.support)
data.temp$visionScore.nonSupport <- GET_SURFACE(results$models$visionScore.time.nonSupport)
```

```{r}
results$plots$visionScore.time.support <- plot_ly(
  x = seq(min(gameInfo$visionScore), mean(gameInfo$visionScore) + 4*sd(gameInfo$visionScore), length.out = 50),
  y = seq(5, 60, length.out = 50),
  z = data.temp$visionScore.support,
  type = "surface"
) %>% 
  layout(
    scene = list(
      xaxis = list(title = "Vision Score"),
      yaxis = list(title = "Game Time (Mins)"),
      zaxis = list(title = "Probability of winning")
    )
  )

results$plots$visionScore.time.support
```
```{r}
results$plots$visionScore.time.nonSupport <- plot_ly(
  x = seq(min(gameInfo$visionScore), mean(gameInfo$visionScore) + 4*sd(gameInfo$visionScore), length.out = 50),
  y = seq(5, 60, length.out = 50),
  z = data.temp$visionScore.nonSupport,
  type = "surface"
) %>% 
  layout(
    scene = list(
      xaxis = list(title = "Vision Score"),
      yaxis = list(title = "Game Time (Mins)"),
      zaxis = list(title = "Probability of winning")
    )
  )

results$plots$visionScore.time.nonSupport
```
## Calculating the Contour of 50% Win Rate
```{r}
data.temp$contour.visionScore.support <- tibble(gameTime = 1:12*5) %>% 
  mutate(visionScore = (-results$models$visionScore.time.support$coefficients[[1]]-results$models$visionScore.time.support$coefficients[[3]]*gameTime)/(results$models$visionScore.time.support$coefficients[[2]] + results$models$visionScore.time.support$coefficients[[4]]*gameTime))
data.temp$contour.visionScore.nonSupport <- tibble(gameTime = 1:12*5) %>% 
  mutate(visionScore = (-results$models$visionScore.time.nonSupport$coefficients[[1]]-results$models$visionScore.time.nonSupport$coefficients[[3]]*gameTime)/(results$models$visionScore.time.nonSupport$coefficients[[2]] + results$models$visionScore.time.nonSupport$coefficients[[4]]*gameTime))

data.temp$contour.visionScore.support
data.temp$contour.visionScore.nonSupport
```

So for a support, you want your vision score to be around 25 at 20 minutes, 45 at 30 minutes, and 80 at 40 minutes to have a 50% chance of winning. For a non-support, you want your vision score to be around 10 at 20 minutes, 20 at 30 minutes, and 30 at 40 minutes.

# Gold Percentage by Position
It's a common idea that the ADC role has been considered useless for this season - let's see what the data has to say about that!
```{r}
data.temp$goldPercent <- gameInfo %>% 
  group_by(match, win) %>% # here win is just used to group team
  mutate(team_gold = sum(goldEarned), goldEarned = goldEarned / team_gold) %>% 
  ungroup()

results$models$goldPercent <- glm(win ~ goldEarned*individualPosition, data = data.temp$goldPercent %>% filter(individualPosition != "Invalid"), family = "binomial")

summary(results$models$goldPercent)
```
```{r}
results$plots$goldPercent <- plot_model(results$models$goldPercent, type = "int")

results$plots$goldPercent 
```
So it would appear that having your carry fed is actually a very strong indicator of win percentage. So why might people believe that ADC is useless? Let's add tier as a covariate and see what we see. (Also as a note, winrate drops off a cliff if support steals all the gold so good to know...as a support main)
## Add tier
```{r}
results$models$goldPercent.tier <- glm(win~goldEarned*individualPosition*tier, data = data.temp$goldPercent %>% filter(individualPosition != "Invalid"), family = "binomial")

summary(results$models$goldPercent.tier)
```
```{r}
results$plots$goldPercent.tier <- plot_model(results$models$goldPercent.tier, type = "int")

results$plots$goldPercent.tier[[4]]
```
Results are unclear, for lower elo's *in general*, gold percentage is less important on ADC but it's not super clear based on the the variance in Silver and Platinum. Interestingly, jungle seems to be more of a carry role in higher elos. Might be because of the champion choice. This will be examined later.

# Total Damage Done
```{r}
results$models$damageDoneChamp <- glm(win~totalDamageDealtToChampions*individualPosition, data = gameInfo.noInvalid, family = "binomial")

summary(results$models$damageDoneChamp)
```
```{r}
results$plots$damageDoneChamp <- plot_model(damageDoneChamp.fit, type = "int")

results$plots$damageDoneChamp
```
Not much difference based on whoever does damage to champion aside from support.
## Average Total Damage and Damage to Champions based on Role
```{r}
data.temp$damageDone <- gameInfo.noInvalid %>% 
  select(individualPosition, totalDamageDealtToChampions, totalDamageDealt)
data.temp$damageDone %>% 
  ggplot(aes(x = individualPosition, y = totalDamageDealt)) +
  geom_boxplot()
data.temp$damageDone %>%  
  ggplot(aes(x = individualPosition, y = totalDamageDealtToChampions)) +
  geom_boxplot()
data.temp$damageDone %>%  
  group_by(individualPosition) %>% 
  summarize(avg_damage_champ = mean(totalDamageDealtToChampions), avg_damage_total = mean(totalDamageDealt))
```
## Kruskal-Wallis Test
```{r}
kruskal.test(totalDamageDealt~individualPosition, data = data.temp$damageDone)
pairwise.wilcox.test(data.temp$damageDone$totalDamageDealt, data.temp$damageDone$individualPosition, p.adjust.method = "fdr")
```
Not really useful, sample size is so large which makes any small difference significant. Visual analysis of boxplots probably more useful.

# Champion Difficulty
```{r}
data.temp$championDifficulty <- gameInfo %>% 
  left_join(
    champions.scraped %>% 
      select(name, difficulty),
    by = c("championName" = "name")
  ) %>% 
  filter(!is.na(difficulty))

results$models$championDifficulty <- glm(win ~ difficulty*tier, data = data.temp$championDifficulty, family = "binomial")

summary(results$models$championDifficulty)
```
```{r}
results$plots$championDifficulty <- plot_model(results$models$championDifficulty, type = "int")

results$plots$championDifficulty
```
Hard to say - I might have expected that win rate of difficult champions would increase when we get to higher ranks. Difficulty however, does not take into account meta and other interactions. The model generated doesn't have the greatest significance so interpretability is limited. It is striking however, the general trend of winrate decreasing with the increase of champion difficulty. Maybe if we interpret difficulty as an ordinal variable we might see a significant linear regression model. Will be looked at in more detail after examining initial winrates in the "ChampionStatistics.RMD" notebook.

A soft takeaway then is play easier champions...at least easier as defined by mobalytics.
